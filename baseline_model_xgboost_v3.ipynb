{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dnG59jvfvjpD",
   "metadata": {
    "id": "dnG59jvfvjpD"
   },
   "source": [
    "# COMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "OFkpGM1-1ywI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "executionInfo": {
     "elapsed": 2980,
     "status": "ok",
     "timestamp": 1759935158887,
     "user": {
      "displayName": "Emam Hossain",
      "userId": "00475912484380842878"
     },
     "user_tz": 240
    },
    "id": "OFkpGM1-1ywI",
    "outputId": "85b77228-f142-4ea0-ea39-445d976d1237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stations: ['Annapolis', 'Atlantic_City', 'Charleston', 'Washington', 'Wilmington', 'Eastport', 'Portland', 'Sewells_Point', 'Sandy_Hook']\n",
      "Testing stations: ['Lewes', 'Fernandina_Beach', 'The_Battery']\n",
      "Number of stations: 12\n",
      "Sea level shape (time x stations): (622392, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>station_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>sea_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1950-01-01 00:00:00.000000</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>1.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1950-01-01 00:59:59.999997</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>1.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1950-01-01 02:00:00.000003</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>1.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1950-01-01 03:00:00.000000</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>1.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1950-01-01 03:59:59.999997</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>1.341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        time station_name  latitude  longitude  sea_level\n",
       "0 1950-01-01 00:00:00.000000    Annapolis  38.98328   -76.4816      1.341\n",
       "1 1950-01-01 00:59:59.999997    Annapolis  38.98328   -76.4816      1.311\n",
       "2 1950-01-01 02:00:00.000003    Annapolis  38.98328   -76.4816      1.280\n",
       "3 1950-01-01 03:00:00.000000    Annapolis  38.98328   -76.4816      1.280\n",
       "4 1950-01-01 03:59:59.999997    Annapolis  38.98328   -76.4816      1.341"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# Install & Import Dependencies\n",
    "# ============================================\n",
    "# !pip install scipy pandas tensorflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Helper Function: MATLAB datenum → datetime\n",
    "# ============================================\n",
    "def matlab2datetime(matlab_datenum):\n",
    "    return datetime.fromordinal(int(matlab_datenum)) \\\n",
    "           + timedelta(days=matlab_datenum % 1) \\\n",
    "           - timedelta(days=366)\n",
    "\n",
    "# ============================================\n",
    "# Load .mat Dataset\n",
    "# ============================================\n",
    "data = loadmat('NEUSTG_19502020_12stations.mat')\n",
    "\n",
    "lat = data['lattg'].flatten()\n",
    "lon = data['lontg'].flatten()\n",
    "sea_level = data['sltg']\n",
    "station_names = [s[0] for s in data['sname'].flatten()]\n",
    "time = data['t'].flatten()\n",
    "time_dt = np.array([matlab2datetime(t) for t in time])\n",
    "\n",
    "# ============================================\n",
    "# Select Target Stations (Training & Testing)\n",
    "# ============================================\n",
    "TRAINING_STATIONS = ['Annapolis','Atlantic_City','Charleston','Washington','Wilmington', 'Eastport', 'Portland', 'Sewells_Point', 'Sandy_Hook']\n",
    "TESTING_STATIONS = ['Lewes', 'Fernandina_Beach', 'The_Battery']\n",
    "\n",
    "# Combine all stations for data loading\n",
    "ALL_STATIONS = TRAINING_STATIONS + TESTING_STATIONS\n",
    "\n",
    "selected_idx = [station_names.index(st) for st in ALL_STATIONS]\n",
    "selected_names = [station_names[i] for i in selected_idx]\n",
    "selected_lat = lat[selected_idx]\n",
    "selected_lon = lon[selected_idx]\n",
    "selected_sea_level = sea_level[:, selected_idx]  # time × selected_stations\n",
    "\n",
    "print(f\"Training stations: {TRAINING_STATIONS}\")\n",
    "print(f\"Testing stations: {TESTING_STATIONS}\")\n",
    "\n",
    "# ============================================\n",
    "# Build Preview DataFrame\n",
    "# ============================================\n",
    "df_preview = pd.DataFrame({\n",
    "    'time': np.tile(time_dt[:5], len(selected_names)),\n",
    "    'station_name': np.repeat(selected_names, 5),\n",
    "    'latitude': np.repeat(selected_lat, 5),\n",
    "    'longitude': np.repeat(selected_lon, 5),\n",
    "    'sea_level': selected_sea_level[:5, :].T.flatten()\n",
    "})\n",
    "\n",
    "# ============================================\n",
    "# Print Data Head\n",
    "# ============================================\n",
    "print(f\"Number of stations: {len(selected_names)}\")\n",
    "print(f\"Sea level shape (time x stations): {selected_sea_level.shape}\")\n",
    "df_preview.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "AMPBvwlk0amI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "executionInfo": {
     "elapsed": 5973,
     "status": "ok",
     "timestamp": 1759935168332,
     "user": {
      "displayName": "Emam Hossain",
      "userId": "00475912484380842878"
     },
     "user_tz": 240
    },
    "id": "AMPBvwlk0amI",
    "outputId": "d63b6a84-1b38-493b-bb60-b3037fe753ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_name</th>\n",
       "      <th>time</th>\n",
       "      <th>sea_level</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>flood_threshold</th>\n",
       "      <th>sea_level_max</th>\n",
       "      <th>flood</th>\n",
       "      <th>sea_level_3d_mean</th>\n",
       "      <th>sea_level_7d_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Annapolis</td>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>2.299667</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>3.945163</td>\n",
       "      <td>6.288</td>\n",
       "      <td>1</td>\n",
       "      <td>2.299667</td>\n",
       "      <td>2.299667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Annapolis</td>\n",
       "      <td>1950-01-02</td>\n",
       "      <td>1.941625</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>3.945163</td>\n",
       "      <td>6.105</td>\n",
       "      <td>1</td>\n",
       "      <td>2.120646</td>\n",
       "      <td>2.120646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annapolis</td>\n",
       "      <td>1950-01-03</td>\n",
       "      <td>1.562000</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>3.945163</td>\n",
       "      <td>4.612</td>\n",
       "      <td>1</td>\n",
       "      <td>1.934431</td>\n",
       "      <td>1.934431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Annapolis</td>\n",
       "      <td>1950-01-04</td>\n",
       "      <td>1.518958</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>3.945163</td>\n",
       "      <td>3.200</td>\n",
       "      <td>0</td>\n",
       "      <td>1.674194</td>\n",
       "      <td>1.830563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Annapolis</td>\n",
       "      <td>1950-01-05</td>\n",
       "      <td>1.922667</td>\n",
       "      <td>38.98328</td>\n",
       "      <td>-76.4816</td>\n",
       "      <td>3.945163</td>\n",
       "      <td>3.414</td>\n",
       "      <td>0</td>\n",
       "      <td>1.667875</td>\n",
       "      <td>1.848983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_name       time  sea_level  latitude  longitude  flood_threshold  \\\n",
       "0    Annapolis 1950-01-01   2.299667  38.98328   -76.4816         3.945163   \n",
       "1    Annapolis 1950-01-02   1.941625  38.98328   -76.4816         3.945163   \n",
       "2    Annapolis 1950-01-03   1.562000  38.98328   -76.4816         3.945163   \n",
       "3    Annapolis 1950-01-04   1.518958  38.98328   -76.4816         3.945163   \n",
       "4    Annapolis 1950-01-05   1.922667  38.98328   -76.4816         3.945163   \n",
       "\n",
       "   sea_level_max  flood  sea_level_3d_mean  sea_level_7d_mean  \n",
       "0          6.288      1           2.299667           2.299667  \n",
       "1          6.105      1           2.120646           2.120646  \n",
       "2          4.612      1           1.934431           1.934431  \n",
       "3          3.200      0           1.674194           1.830563  \n",
       "4          3.414      0           1.667875           1.848983  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# Convert Hourly → Daily per Station\n",
    "# ============================================\n",
    "# Convert time to pandas datetime\n",
    "time_dt = pd.to_datetime(time_dt)\n",
    "\n",
    "# Build hourly DataFrame for selected stations\n",
    "df_hourly = pd.DataFrame({\n",
    "    'time': np.tile(time_dt, len(selected_names)),\n",
    "    'station_name': np.repeat(selected_names, len(time_dt)),\n",
    "    'latitude': np.repeat(selected_lat, len(time_dt)),\n",
    "    'longitude': np.repeat(selected_lon, len(time_dt)),\n",
    "    'sea_level': selected_sea_level.flatten()\n",
    "})\n",
    "\n",
    "# ============================================\n",
    "# Compute Flood Threshold per Station\n",
    "# ============================================\n",
    "threshold_df = df_hourly.groupby('station_name')['sea_level'].agg(['mean','std']).reset_index()\n",
    "threshold_df['flood_threshold'] = threshold_df['mean'] + 1.5 * threshold_df['std']\n",
    "\n",
    "df_hourly = df_hourly.merge(threshold_df[['station_name','flood_threshold']], on='station_name', how='left')\n",
    "\n",
    "# ============================================\n",
    "# Daily Aggregation + Flood Flag\n",
    "# ============================================\n",
    "df_daily = df_hourly.groupby(['station_name', pd.Grouper(key='time', freq='D')]).agg({\n",
    "    'sea_level': 'mean',\n",
    "    'latitude': 'first',\n",
    "    'longitude': 'first',\n",
    "    'flood_threshold': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Flood flag: 1 if any hourly value exceeded threshold that day\n",
    "hourly_max = df_hourly.groupby(['station_name', pd.Grouper(key='time', freq='D')])['sea_level'].max().reset_index()\n",
    "df_daily = df_daily.merge(hourly_max, on=['station_name','time'], suffixes=('','_max'))\n",
    "df_daily['flood'] = (df_daily['sea_level_max'] > df_daily['flood_threshold']).astype(int)\n",
    "\n",
    "# ============================================\n",
    "# Feature Engineering (3d & 7d means)\n",
    "# ============================================\n",
    "df_daily['sea_level_3d_mean'] = df_daily.groupby('station_name')['sea_level'].transform(\n",
    "    lambda x: x.rolling(3, min_periods=1).mean())\n",
    "df_daily['sea_level_7d_mean'] = df_daily.groupby('station_name')['sea_level'].transform(\n",
    "    lambda x: x.rolling(7, min_periods=1).mean())\n",
    "\n",
    "# Preview\n",
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7PJExCrs1-6q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79617,
     "status": "ok",
     "timestamp": 1759935250798,
     "user": {
      "displayName": "Emam Hossain",
      "userId": "00475912484380842878"
     },
     "user_tz": 240
    },
    "id": "7PJExCrs1-6q",
    "outputId": "fe57faee-8333-45c9-a81e-30cf3e7bb8b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (233208, 21)\n",
      "y_train shape: (233208, 14)\n",
      "Training data from 9 training stations\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Build 7-day → 14-day Training Windows\n",
    "# ============================================\n",
    "FEATURES = ['sea_level', 'sea_level_3d_mean', 'sea_level_7d_mean']\n",
    "HIST_DAYS = 7\n",
    "FUTURE_DAYS = 14\n",
    "\n",
    "X_train, y_train = [], []\n",
    "\n",
    "for stn, grp in df_daily.groupby('station_name'):\n",
    "    grp = grp.sort_values('time').reset_index(drop=True)\n",
    "    for i in range(len(grp) - HIST_DAYS - FUTURE_DAYS):\n",
    "        hist = grp.loc[i:i+HIST_DAYS-1, FEATURES].values.flatten()\n",
    "        future = grp.loc[i+HIST_DAYS:i+HIST_DAYS+FUTURE_DAYS-1, 'flood'].values\n",
    "        X_train.append(hist)\n",
    "        y_train.append(future)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bJZ1pEQ1-85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1759935255061,
     "user": {
      "displayName": "Emam Hossain",
      "userId": "00475912484380842878"
     },
     "user_tz": 240
    },
    "id": "6bJZ1pEQ1-85",
    "outputId": "bad6f323-ba2a-452f-a5c7-d4eb3368b7f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 prediction windows\n",
      "\n",
      "Prediction windows:\n",
      "   start_date   end_date\n",
      "0  1962-03-06 1962-03-12\n",
      "1  2013-07-21 2013-07-27\n",
      "2  2011-05-13 2011-05-19\n",
      "3  1995-12-21 1995-12-27\n",
      "4  1995-09-05 1995-09-11\n",
      "5  2009-12-31 2010-01-06\n",
      "6  2020-09-16 2020-09-22\n",
      "7  2013-10-07 2013-10-13\n",
      "8  1958-04-03 1958-04-09\n",
      "9  2011-05-13 2011-05-19\n",
      "10 1988-04-08 1988-04-14\n",
      "11 1996-12-04 1996-12-10\n",
      "12 2003-04-14 2003-04-20\n",
      "13 1979-01-25 1979-01-31\n",
      "14 2015-03-18 2015-03-24\n",
      "\n",
      "X_test shape: (15, 3, 21)  (windows × stations × 21 features)\n",
      "Number of valid windows: 15\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Load Prediction Windows from Seed File\n",
    "# ============================================\n",
    "# Read prediction windows from Seed_Historical_Time_Intervals.txt\n",
    "pred_windows_df = pd.read_csv('Seed_Historical_Time_Intervals.txt', sep='\\t')\n",
    "pred_windows_df['start_date'] = pd.to_datetime(pred_windows_df['start_date'])\n",
    "pred_windows_df['end_date'] = pd.to_datetime(pred_windows_df['end_date'])\n",
    "\n",
    "print(f\"Loaded {len(pred_windows_df)} prediction windows\")\n",
    "print(\"\\nPrediction windows:\")\n",
    "print(pred_windows_df)\n",
    "\n",
    "# ============================================\n",
    "# Build X_test for All Windows\n",
    "# ============================================\n",
    "FEATURES = ['sea_level', 'sea_level_3d_mean', 'sea_level_7d_mean']\n",
    "X_test_all = []\n",
    "test_windows_info = []\n",
    "\n",
    "for idx, row in pred_windows_df.iterrows():\n",
    "    hist_start = row['start_date']\n",
    "    hist_end = row['end_date']\n",
    "    \n",
    "    # Forecast period (14 days after historical window)\n",
    "    test_start = hist_end + pd.Timedelta(days=1)\n",
    "    test_end = test_start + pd.Timedelta(days=13)\n",
    "    \n",
    "    # Build X_test for this window (TESTING_STATIONS only)\n",
    "    X_test_window = []\n",
    "    for stn, grp in df_daily.groupby('station_name'):\n",
    "        if stn in TESTING_STATIONS:\n",
    "            mask = (grp['time'] >= hist_start) & (grp['time'] <= hist_end)\n",
    "            hist_block = grp.loc[mask, FEATURES].values.flatten()\n",
    "            if len(hist_block) == 7 * len(FEATURES):   # ensure full 7-day block\n",
    "                X_test_window.append(hist_block)\n",
    "    \n",
    "    if len(X_test_window) > 0:\n",
    "        X_test_all.append(np.array(X_test_window))\n",
    "        test_windows_info.append({\n",
    "            'window_idx': idx,\n",
    "            'hist_start': hist_start,\n",
    "            'hist_end': hist_end,\n",
    "            'test_start': test_start,\n",
    "            'test_end': test_end\n",
    "        })\n",
    "\n",
    "X_test = np.array(X_test_all)  # Shape: (num_windows, num_stations, 21)\n",
    "print(f\"\\nX_test shape: {X_test.shape}  (windows × stations × {7*len(FEATURES)} features)\")\n",
    "print(f\"Number of valid windows: {len(test_windows_info)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3f73448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.1583, Val Loss: 0.0991\n",
      "Epoch [2/20], Train Loss: 0.1142, Val Loss: 0.0952\n",
      "Epoch [4/20], Train Loss: 0.1078, Val Loss: 0.0905\n",
      "Epoch [6/20], Train Loss: 0.1053, Val Loss: 0.0904\n",
      "Epoch [8/20], Train Loss: 0.1041, Val Loss: 0.0882\n",
      "Epoch [10/20], Train Loss: 0.1032, Val Loss: 0.0868\n",
      "Epoch [12/20], Train Loss: 0.1025, Val Loss: 0.0865\n",
      "Epoch [14/20], Train Loss: 0.1017, Val Loss: 0.0871\n",
      "Epoch [16/20], Train Loss: 0.1012, Val Loss: 0.0884\n",
      "Epoch [18/20], Train Loss: 0.1010, Val Loss: 0.0851\n",
      "Epoch [20/20], Train Loss: 0.1006, Val Loss: 0.0865\n",
      "\n",
      "=== Improved LSTM Results (PyTorch) - All Windows ===\n",
      "Total predictions: 630 (across 15 windows × 3 testing stations × 14 days)\n",
      "Testing stations: ['Lewes', 'Fernandina_Beach', 'The_Battery']\n",
      "TP: 404 | FP: 8 | TN: 216 | FN: 2\n",
      "Accuracy: 0.984 | F1: 0.988 | MCC: 0.965\n",
      "\n",
      "=== Per-Window Metrics ===\n",
      "Window 1 (1962-03-06 - 1962-03-12): Acc=1.000, F1=1.000\n",
      "Window 2 (2013-07-21 - 2013-07-27): Acc=0.976, F1=0.982\n",
      "Window 3 (2011-05-13 - 2011-05-19): Acc=1.000, F1=1.000\n",
      "Window 4 (1995-12-21 - 1995-12-27): Acc=1.000, F1=1.000\n",
      "Window 5 (1995-09-05 - 1995-09-11): Acc=1.000, F1=1.000\n",
      "Window 6 (2009-12-31 - 2010-01-06): Acc=1.000, F1=1.000\n",
      "Window 7 (2020-09-16 - 2020-09-22): Acc=0.952, F1=0.960\n",
      "Window 8 (2013-10-07 - 2013-10-13): Acc=0.976, F1=0.981\n",
      "Window 9 (1958-04-03 - 1958-04-09): Acc=0.929, F1=0.943\n",
      "Window 10 (2011-05-13 - 2011-05-19): Acc=1.000, F1=1.000\n",
      "Window 11 (1988-04-08 - 1988-04-14): Acc=1.000, F1=1.000\n",
      "Window 12 (1996-12-04 - 1996-12-10): Acc=0.976, F1=0.982\n",
      "Window 13 (2003-04-14 - 2003-04-20): Acc=0.976, F1=0.982\n",
      "Window 14 (1979-01-25 - 1979-01-31): Acc=1.000, F1=1.000\n",
      "Window 15 (2015-03-18 - 2015-03-24): Acc=0.976, F1=0.981\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Improved LSTM Model Implementation (PyTorch)\n",
    "# ============================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, matthews_corrcoef\n",
    "\n",
    "# Reshape data for LSTM: (samples, timesteps, features)\n",
    "FEATURES = ['sea_level', 'sea_level_3d_mean', 'sea_level_7d_mean']\n",
    "X_train_lstm = X_train.reshape(-1, 7, len(FEATURES))\n",
    "\n",
    "# X_test is now 3D: (num_windows, num_stations, 21 features)\n",
    "# Reshape to (num_windows * num_stations, 7, 3) for LSTM\n",
    "X_test_lstm = X_test.reshape(-1, 7, len(FEATURES))\n",
    "\n",
    "# Normalize features (per feature across all timesteps and samples)\n",
    "X_train_reshaped = X_train_lstm.reshape(-1, len(FEATURES))\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped).reshape(X_train_lstm.shape)\n",
    "\n",
    "X_test_reshaped = X_test_lstm.reshape(-1, len(FEATURES))\n",
    "X_test_scaled = scaler.transform(X_test_reshaped).reshape(X_test_lstm.shape)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "\n",
    "# Improved PyTorch LSTM model with deeper architecture\n",
    "class ImprovedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=64, num_layers=2, dropout=0.3, output_size=14):\n",
    "        super(ImprovedLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Multi-layer LSTM with dropout\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Additional fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bn = nn.BatchNorm1d(hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Take the last output from LSTM\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers with batch norm and dropout\n",
    "        out = self.fc1(last_out)\n",
    "        out = self.bn(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# Initialize improved model\n",
    "lstm_model = ImprovedLSTMModel(\n",
    "    input_size=len(FEATURES), \n",
    "    hidden_size=64, \n",
    "    num_layers=2,\n",
    "    dropout=0.3,\n",
    "    output_size=14\n",
    ")\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Validation split\n",
    "val_size = int(len(X_train_tensor) * 0.1)\n",
    "train_size = len(X_train_tensor) - val_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Training with early stopping\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    lstm_model.train()\n",
    "    train_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation phase\n",
    "    lstm_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            outputs = lstm_model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(lstm_model.state_dict(), 'best_lstm_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        # Load best model\n",
    "        lstm_model.load_state_dict(torch.load('best_lstm_model.pth'))\n",
    "        break\n",
    "\n",
    "# Load best model for prediction\n",
    "try:\n",
    "    lstm_model.load_state_dict(torch.load('best_lstm_model.pth'))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Predict for all windows\n",
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_lstm = lstm_model(X_test_tensor).numpy()\n",
    "y_pred_lstm_bin = (y_pred_lstm > 0.5).astype(int)\n",
    "\n",
    "# Reshape predictions back to (num_windows, num_stations, 14)\n",
    "num_stations = len(TESTING_STATIONS)\n",
    "num_windows = len(test_windows_info)\n",
    "y_pred_lstm_bin = y_pred_lstm_bin.reshape(num_windows, num_stations, 14)\n",
    "\n",
    "# Collect ground truth for all windows (TESTING_STATIONS only)\n",
    "y_true_all = []\n",
    "for window_info in test_windows_info:\n",
    "    test_start = window_info['test_start']\n",
    "    test_end = window_info['test_end']\n",
    "    y_true_window = []\n",
    "    for stn, grp in df_daily.groupby('station_name'):\n",
    "        if stn in TESTING_STATIONS:\n",
    "            mask = (grp['time'] >= test_start) & (grp['time'] <= test_end)\n",
    "            vals = grp.loc[mask, 'flood'].values\n",
    "            if len(vals) == 14:\n",
    "                y_true_window.append(vals)\n",
    "            else:\n",
    "                # Pad with zeros if missing days\n",
    "                y_true_window.append(np.zeros(14, dtype=int))\n",
    "    y_true_all.append(np.array(y_true_window))\n",
    "\n",
    "y_true_all = np.array(y_true_all)  # Shape: (num_windows, num_stations, 14)\n",
    "\n",
    "# Aggregate results across all windows\n",
    "y_true_flat = y_true_all.flatten()\n",
    "y_pred_flat = y_pred_lstm_bin.flatten()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true_flat, y_pred_flat).ravel()\n",
    "acc = accuracy_score(y_true_flat, y_pred_flat)\n",
    "f1 = f1_score(y_true_flat, y_pred_flat)\n",
    "mcc = matthews_corrcoef(y_true_flat, y_pred_flat)\n",
    "\n",
    "print(\"\\n=== Improved LSTM Results (PyTorch) - All Windows ===\")\n",
    "print(f\"Total predictions: {len(y_true_flat)} (across {num_windows} windows × {num_stations} testing stations × 14 days)\")\n",
    "print(f\"Testing stations: {TESTING_STATIONS}\")\n",
    "print(f\"TP: {tp} | FP: {fp} | TN: {tn} | FN: {fn}\")\n",
    "print(f\"Accuracy: {acc:.3f} | F1: {f1:.3f} | MCC: {mcc:.3f}\")\n",
    "\n",
    "# Per-window metrics\n",
    "print(\"\\n=== Per-Window Metrics ===\")\n",
    "for i, window_info in enumerate(test_windows_info):\n",
    "    y_true_win = y_true_all[i].flatten()\n",
    "    y_pred_win = y_pred_lstm_bin[i].flatten()\n",
    "    acc_win = accuracy_score(y_true_win, y_pred_win)\n",
    "    f1_win = f1_score(y_true_win, y_pred_win)\n",
    "    print(f\"Window {i+1} ({window_info['hist_start'].date()} - {window_info['hist_end'].date()}): \"\n",
    "          f\"Acc={acc_win:.3f}, F1={f1_win:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "agentic-ai-platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
